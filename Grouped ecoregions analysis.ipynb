{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97558737-04d4-41bd-b753-1f7bbef55c7c",
   "metadata": {},
   "source": [
    "Notebook which tests across ecoregions for normality using the Shapiro-Wilk test, conducts a Mann-Kendall trend analysis, makes seasonality heatmaps (use only for normally-distributed data), moving window monthly boxplots, finds the annual IQR and the max month of burn.\n",
    "\n",
    "Ecoregions are grouped by North America boreal, Eurasia boreal, and tundra (all).\n",
    "\n",
    "Edit as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81661346-8abc-491a-99ac-55430f84e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymannkendall as mk\n",
    "from scipy.stats import shapiro\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fbdf4-6d96-4cbf-b867-9edcfa950cfd",
   "metadata": {},
   "source": [
    "## Grouped regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c3c32-7edb-4b76-ac9d-b096dbd7adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load shapefile\n",
    "shp_path = '/home/users/clelland/Model/Analysis/RESOLVE shapefile from GEE/resolve_shapefile_from_gee.shp'\n",
    "gdf = gpd.read_file(shp_path)\n",
    "gdf = gdf.to_crs(epsg=6931)\n",
    "selected_ecoregions = gdf[gdf['BIOME_NAME'].isin(['Boreal Forests/Taiga', 'Tundra'])].reset_index(drop=True)\n",
    "\n",
    "world_path = '/home/users/clelland/Model/Analysis/Countries shapefile/world-administrative-boundaries.shp'\n",
    "gdf_world = gpd.read_file(world_path)\n",
    "gdf_world = gdf_world.to_crs(epsg=6931)\n",
    "\n",
    "# List of region names — must match order of selected_ecoregions\n",
    "region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr'), ('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "               ('scanand', 'scrusta'), ('uralmon', 'uralfor'), ('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')]\n",
    "\n",
    "# Model/scenario combinations\n",
    "models = ['access 126', 'access 245', 'access 370', 'mri 126', 'mri 245', 'mri 370']\n",
    "model_labels = {\n",
    "    'access 126': 'ACCESS_SSP126',\n",
    "    'access 245': 'ACCESS_SSP245',\n",
    "    'access 370': 'ACCESS_SSP370',\n",
    "    'mri 126': 'MRI_SSP126',\n",
    "    'mri 245': 'MRI_SSP245',\n",
    "    'mri 370': 'MRI_SSP370'\n",
    "}\n",
    "\n",
    "# Time periods for projections\n",
    "periods = {\n",
    "    '2025_2050': ('2025', '2050'),\n",
    "    '2051_2075': ('2051', '2075'),\n",
    "    '2076_2100': ('2076', '2100')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3b829-1f92-44fd-b72f-d5e676306de3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Shapiro-Wilk test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b9e01-71f8-425b-a4b9-b1374354369a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignore large N warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"scipy.stats.shapiro: For N > 5000, computed p-value may not be accurate.\")\n",
    "\n",
    "for region_actual, region_model in region_mappings:\n",
    "    actual_path = '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv'\n",
    "    model_path = f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv'\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Missing model file for region: {region_model}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_actual = pd.read_csv(actual_path, parse_dates=['date'], index_col='date')\n",
    "        df_model = pd.read_csv(model_path, parse_dates=['time'], index_col='time')\n",
    "\n",
    "        print(f\"\\nRegion: {region_actual} / {region_model}\")\n",
    "\n",
    "        # Run Shapiro-Wilk test for all columns in actual data\n",
    "        if region_actual in df_actual.columns:\n",
    "            data = df_actual[region_actual].dropna()\n",
    "            stat, p = shapiro(data)\n",
    "            print(f\"  Actual - {region_actual}: W={stat:.4f}, p={p:.4g} → {'Normal' if p > 0.05 else 'Not normal'}\")\n",
    "        else:\n",
    "            for col in df_actual.columns:\n",
    "                data = df_actual[col].dropna()\n",
    "                if len(data) > 3:\n",
    "                    stat, p = shapiro(data)\n",
    "                    print(f\"  Actual - {col}: W={stat:.4f}, p={p:.4g} → {'Normal' if p > 0.05 else 'Not normal'}\")\n",
    "\n",
    "        # Run Shapiro-Wilk test for all columns in model data\n",
    "        for col in df_model.columns:\n",
    "            data = df_model[col].dropna()\n",
    "            if len(data) > 3:\n",
    "                stat, p = shapiro(data)\n",
    "                print(f\"  Model  - {col}: W={stat:.4f}, p={p:.4g} → {'Normal' if p > 0.05 else 'Not normal'}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing region {region_actual}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187cd4c-584c-4eb8-befd-56b070556acc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Mann-Kendall trend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182943a2-a620-4114-b14c-421ec3c3cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Kendall trend analysis - whole time period\n",
    "#region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "#               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "#               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "#region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "#               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "#               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "#               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "#               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "#               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "# Step 1: Load all region files into a list of DataFrames\n",
    "models = ['ACCESS_SSP126', 'ACCESS_SSP245', 'ACCESS_SSP370',\n",
    "          'MRI_SSP126', 'MRI_SSP245', 'MRI_SSP370']\n",
    "\n",
    "# Placeholder for BA data\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    try:\n",
    "        # Load actual data\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled data\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Resample to annual and apply scaling\n",
    "        df_model_annual = df_model.resample('YE').sum().astype(float)\n",
    "\n",
    "        # Resample actual to annual\n",
    "        df_actual_annual = df_actual.resample('YE').sum()\n",
    "\n",
    "        # Combine both\n",
    "        df_ba = pd.concat([df_actual_annual, df_model_annual], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Combine all regions and sum across them\n",
    "df_ba_all = pd.concat(ba_dfs, axis=0).groupby(level=0).sum(min_count=1)\n",
    "df_ba_all.index = pd.to_datetime(df_ba_all.index)\n",
    "df_ba_all.sort_index(inplace=True)\n",
    "\n",
    "periods = {\n",
    "    '2025-2050': (2025, 2050),\n",
    "    '2051-2075': (2051, 2075),\n",
    "    '2076-2100': (2076, 2100),\n",
    "}\n",
    "\n",
    "trend_records_full = []\n",
    "\n",
    "for model in models:\n",
    "    if model not in df_ba_all.columns:\n",
    "        continue\n",
    "\n",
    "    df_model = df_ba_all[[model]].copy()\n",
    "    df_model['year'] = df_model.index.year\n",
    "    df_annual = df_model.groupby('year')[model].sum().reset_index()\n",
    "\n",
    "    if len(df_annual) < 10:\n",
    "        print(f\"Skipping {model}: insufficient data\")\n",
    "        continue\n",
    "\n",
    "    result = mk.original_test(df_annual[model].values)\n",
    "\n",
    "    trend_records_full.append({\n",
    "        'Model': model,\n",
    "        'Period': '2025–2100',\n",
    "        'Trend': result.trend,\n",
    "        'H': result.h,\n",
    "        'PValue': result.p,\n",
    "        'Tau': result.Tau,\n",
    "        'Slope': result.slope\n",
    "    })\n",
    "\n",
    "# Compile results\n",
    "df_mk_full = pd.DataFrame(trend_records_full)\n",
    "df_mk_full = df_mk_full[['Model', 'Period', 'Trend', 'H', 'PValue', 'Tau', 'Slope']]\n",
    "\n",
    "# Save or display\n",
    "#df_mk_full.to_csv('/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/mk_trends_2025_2100_full.csv', index=False)\n",
    "df_mk_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44f103-d5a0-45cd-a416-3213f0836dc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Yearly/decadal heatmaps - ONLY USE FOR NORMALLY-DISTRIBUTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3f9dc-3a29-4b17-9cd1-49a08aee0909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seasonality by year heatmap\n",
    "#region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "#               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "#               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "#region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "#               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "#               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "#               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "#               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "#               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "# List to store individual region DataFrames\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    root = f'/home/users/clelland/Model/Analysis/CMIP and FWI time series/Ecoregion CSVs/{region}'\n",
    "\n",
    "    try:\n",
    "        # Load actual burned area\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled burned area\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Combine actual and model\n",
    "        df_ba = pd.concat([df_actual, df_model], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Combine and sum across all regions by timestamp\n",
    "df_ba_all = pd.concat(ba_dfs, axis=0).groupby(level=0).sum(min_count=1)\n",
    "df_ba_all.index = pd.to_datetime(df_ba_all.index)\n",
    "df_ba_all.sort_index(inplace=True)\n",
    "\n",
    "# Create heatmaps for each model\n",
    "model_columns = [col for col in df_ba_all.columns if col.startswith('Model')]\n",
    "heatmap_data_by_model = {}\n",
    "\n",
    "for burn_column in model_columns:\n",
    "    df_season = df_ba_all.copy().reset_index().rename(columns={'index': 'time'})\n",
    "    df_season['time'] = pd.to_datetime(df_season['time'])\n",
    "    df_season.set_index('time', inplace=True)\n",
    "    df_season = df_season.loc['2025-01-01':]\n",
    "\n",
    "    df_season['year'] = df_season.index.year\n",
    "    df_season['month'] = df_season.index.month\n",
    "\n",
    "    # Total burn per year\n",
    "    yearly_totals = df_season.groupby(['year'])[burn_column].sum()\n",
    "\n",
    "    # Monthly burn\n",
    "    monthly_burn = df_season.groupby(['year', 'month'])[burn_column].sum()\n",
    "\n",
    "    # Convert to DataFrame for easier handling\n",
    "    monthly_burn_df = monthly_burn.reset_index()\n",
    "    monthly_burn_df['yearly_total'] = monthly_burn_df['year'].map(yearly_totals)\n",
    "\n",
    "    # Compute percent\n",
    "    monthly_burn_df['percent'] = monthly_burn_df[burn_column] / monthly_burn_df['yearly_total'] * 100\n",
    "\n",
    "    # Add date for plotting\n",
    "    monthly_burn_df['date'] = pd.to_datetime(dict(year=monthly_burn_df['year'],\n",
    "                                                  month=monthly_burn_df['month'],\n",
    "                                                  day=15))\n",
    "    monthly_burn_df.set_index('date', inplace=True)\n",
    "\n",
    "    # Pivot the data to have years as columns, months as rows\n",
    "    heatmap_data = monthly_burn_df.pivot(index='month', columns='year', values='percent')\n",
    "    heatmap_data_by_model[burn_column] = heatmap_data\n",
    "\n",
    "# Plot the heatmaps\n",
    "for model in model_columns:\n",
    "    heatmap_data = heatmap_data_by_model[model]\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.heatmap(heatmap_data, cmap='YlOrRd', linewidths=0.5, linecolor='grey', annot=False, fmt=\".1f\",\n",
    "                cbar_kws={'label': '% of Yearly Burn'})\n",
    "    \n",
    "    plt.yticks(ticks=np.arange(12) + 0.5, labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                                                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=0)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Month')\n",
    "    plt.title(f'Monthly Contribution to Yearly Burn (2025–2100) for Tundra ecoregions - {model}')\n",
    "    plt.tight_layout()\n",
    "    out_path = f'/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/Seasonality by year/ba_seasonal_heatmap_tundra_{model}.png'\n",
    "    #plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f666e0-5736-4de8-985c-d1d0455aea1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seasonality by decade heatmap\n",
    "# Can't use mean and standard deviation as data are not normally distributed\n",
    "#region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "#               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "#               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "#region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "#               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "#               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "#               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "#               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "#               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "models = ['ACCESS_SSP126', 'ACCESS_SSP245', 'ACCESS_SSP370',\n",
    "          'MRI_SSP126', 'MRI_SSP245', 'MRI_SSP370']\n",
    "\n",
    "# Step 1: Load all region files into a list of DataFrames\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    try:\n",
    "        # Load actual burned area\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled burned area\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Combine actual and model\n",
    "        df_ba = pd.concat([df_actual, df_model], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Step 2: Combine all DataFrames by aligning on time index\n",
    "df_ba_all = pd.concat(ba_dfs, axis=1)\n",
    "\n",
    "# Step 3: Sum each model's columns across regions\n",
    "df_ba = pd.DataFrame(index=df_ba_all.index)\n",
    "\n",
    "for model in models:\n",
    "    # Get all columns corresponding to this model from different regions\n",
    "    model_cols = [col for col in df_ba_all.columns if col == model]\n",
    "    if not model_cols:\n",
    "        continue\n",
    "    df_ba[model] = df_ba_all[model_cols].sum(axis=1)\n",
    "\n",
    "# Step 4: Now plot monthly mean ± std dev per decade for each model\n",
    "for model in models:\n",
    "    if model not in df_ba.columns:\n",
    "        continue\n",
    "\n",
    "    burn_column = model\n",
    "\n",
    "    df_season = df_ba[[burn_column]].copy().reset_index().rename(columns={'index':'time'})\n",
    "    df_season['time'] = pd.to_datetime(df_season['time'])\n",
    "    df_season.set_index('time', inplace=True)\n",
    "    df_season = df_season.loc['2025-01-01':]\n",
    "\n",
    "    df_season['year'] = df_season.index.year\n",
    "    df_season['month'] = df_season.index.month\n",
    "    df_season['decade'] = ((df_season['year'] - 1) // 10) * 10 + 1\n",
    "\n",
    "    # Group by month and year for each decade\n",
    "    monthly_grouped = df_season.groupby(['decade', 'year', 'month'])[burn_column].sum()\n",
    "    yearly_grouped = df_season.groupby(['decade', 'year'])[burn_column].sum()\n",
    "\n",
    "    monthly_percent = monthly_grouped / yearly_grouped.loc[monthly_grouped.index.droplevel('month')] * 100\n",
    "\n",
    "    mean_percent = monthly_percent.groupby(['decade', 'month']).mean()\n",
    "    std_percent = monthly_percent.groupby(['decade', 'month']).std()\n",
    "\n",
    "    # Merge into one DataFrame\n",
    "    mean_df = mean_percent.reset_index()\n",
    "    std_df = std_percent.reset_index()\n",
    "    combined_df = pd.merge(mean_df, std_df, on=['decade', 'month'], suffixes=('_mean', '_std'))\n",
    "\n",
    "    # Pivot for plotting\n",
    "    mean_pivot = combined_df.pivot(index='month', columns='decade', values=f'{burn_column}_mean')\n",
    "    std_pivot = combined_df.pivot(index='month', columns='decade', values=f'{burn_column}_std')\n",
    "\n",
    "    formatted_columns = [f\"{int(dec) - 1}s\" for dec in mean_pivot.columns]\n",
    "    mean_pivot.columns = formatted_columns\n",
    "    std_pivot.columns = formatted_columns\n",
    "\n",
    "    # Round mean and std for formatting\n",
    "    mean_rounded = mean_pivot.round(1)\n",
    "    std_rounded = std_pivot.round(1)\n",
    "    \n",
    "    # Create annotation DataFrame with \"mean ± std\", but use \"0\" if both are 0\n",
    "    annot = mean_rounded.astype(str) + \" ± \" + std_rounded.astype(str)\n",
    "    annot[(mean_rounded == 0) & (std_rounded == 0)] = \"0\"\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(mean_pivot, cmap='YlOrRd', annot=annot, fmt='', linewidths=0.5, linecolor='grey',\n",
    "                cbar_kws={'label': 'Mean % of Yearly Burn'})\n",
    "\n",
    "    plt.yticks(ticks=np.arange(12) + 0.5, labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                                                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=0)\n",
    "    plt.xlabel('Decade')\n",
    "    plt.ylabel('Month')\n",
    "    plt.title(f'Monthly Mean ± Std Dev Burn (% of Yearly) by Decade\\nEurasia boreal ecoregions - {model}')\n",
    "    plt.tight_layout()\n",
    "    out_path = f'/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/Seasonality by decade/ba_seasonal_decade_with_sd_EUbor_{model}.png'\n",
    "    #plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0764d8a-00c7-45ed-8874-26f505a7169e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Moving window monthly boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2e5c5-4b2d-416e-a903-5d18e9db661a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Moving window monthly boxplots - NOT WINTER\n",
    "region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "#region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "#               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "#               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "#region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "#               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "#               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "#               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "#               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "#               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "models = ['ACCESS_SSP126', 'ACCESS_SSP245', 'ACCESS_SSP370',\n",
    "          'MRI_SSP126', 'MRI_SSP245', 'MRI_SSP370']\n",
    "\n",
    "# Step 1: Load all region files into a list of DataFrames\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    try:\n",
    "        # Load actual burned area\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled burned area\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Combine actual and model\n",
    "        df_ba = pd.concat([df_actual, df_model], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Step 2: Combine all DataFrames by aligning on time index\n",
    "df_ba_all = pd.concat(ba_dfs, axis=1)\n",
    "\n",
    "# Step 3: Sum each model's columns across regions\n",
    "df_ba = pd.DataFrame(index=df_ba_all.index)\n",
    "\n",
    "for model in models:\n",
    "    # Get all columns corresponding to this model from different regions\n",
    "    model_cols = [col for col in df_ba_all.columns if col == model]\n",
    "    if not model_cols:\n",
    "        continue\n",
    "    df_ba[model] = df_ba_all[model_cols].sum(axis=1)\n",
    "\n",
    "window_size = 30\n",
    "\n",
    "for model in models:\n",
    "#for model in ['ACCESS_SSP126']:\n",
    "    if model not in df_ba.columns:\n",
    "        continue\n",
    "\n",
    "    df_season = df_ba[[model]].copy().reset_index().rename(columns={'index': 'time'})\n",
    "    df_season['time'] = pd.to_datetime(df_season['time'])\n",
    "    df_season.set_index('time', inplace=True)\n",
    "    df_season = df_season.loc['2025-01-01':]\n",
    "\n",
    "    df_season['year'] = df_season.index.year\n",
    "    df_season['month'] = df_season.index.month\n",
    "\n",
    "    # Group by year and month\n",
    "    monthly_grouped = df_season.groupby(['year', 'month'])[model].sum()\n",
    "    yearly_grouped = df_season.groupby('year')[model].sum()\n",
    "\n",
    "    # Normalize by year total to get percentage\n",
    "    monthly_percent = monthly_grouped / monthly_grouped.index.get_level_values('year').map(yearly_grouped) * 100\n",
    "    monthly_percent = pd.DataFrame({model: monthly_percent}).reset_index()\n",
    "\n",
    "    # Months to include (March to November)\n",
    "    months_to_plot = list(range(3, 12))\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 10), sharey=True, sharex=True)\n",
    "    fig.suptitle(f'Seasonality Boxplots (30-yr Moving Window)\\nNorth America boreal ecoregions - {model}', fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "    for idx, month in enumerate(months_to_plot):\n",
    "        ax = axes[idx]\n",
    "        data_month = monthly_percent[monthly_percent['month'] == month].copy()\n",
    "    \n",
    "        # Build sliding windows\n",
    "        box_data = []\n",
    "        window_labels = []\n",
    "        for start_year in range(data_month['year'].min(), data_month['year'].max() - window_size + 1, 5): # 5-yearly\n",
    "            window = data_month[(data_month['year'] >= start_year) & (data_month['year'] < start_year + window_size)]\n",
    "            if len(window) == window_size:\n",
    "                box_data.append(window[model].values)\n",
    "                window_labels.append(f\"{start_year}-{start_year + window_size - 1}\")\n",
    "\n",
    "        # Boxplot with transparent boxes and blue median line\n",
    "        box = ax.boxplot(box_data, labels=window_labels, patch_artist=True,\n",
    "                         flierprops={'marker': 'o', 'markersize': 5})\n",
    "\n",
    "        for patch in box['boxes']:\n",
    "            patch.set(facecolor='none', edgecolor='black', linewidth=1)\n",
    "\n",
    "        for median in box['medians']:\n",
    "            median.set(color='blue', linewidth=2)\n",
    "\n",
    "    \n",
    "        ax.set_title(month_names[month - 1], fontsize=14)\n",
    "        ax.set_ylabel('% of Annual Burn' if idx % 3 == 0 else '', fontsize=14)\n",
    "        ax.set_ylim(-5, 100)\n",
    "        \n",
    "        # Set xticks every 5 labels only\n",
    "        xticks = np.arange(1, len(box_data) + 1)\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(window_labels, rotation=45, ha='right', fontsize=13)\n",
    "    \n",
    "    # Remove any unused axes (e.g., if fewer than 9 months due to data constraints)\n",
    "    for ax in axes[len(months_to_plot):]:\n",
    "        fig.delaxes(ax)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    out_path = f'/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/Moving window boxplots/ba_seasonal_boxplot_NAbor_{model}.png'\n",
    "    #plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bcba2-3c2d-4c1f-9192-7815450e4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find medians\n",
    "window_size = 30\n",
    "step = 5\n",
    "months_to_include = list(range(3, 12))  # March–November\n",
    "summary_medians = []\n",
    "\n",
    "for model in models:\n",
    "    if model not in df_ba.columns:\n",
    "        continue\n",
    "\n",
    "    df_model = df_ba[[model]].copy()\n",
    "    df_model = df_model.loc['2025-01-01':]\n",
    "    df_model['year'] = df_model.index.year\n",
    "    df_model['month'] = df_model.index.month\n",
    "\n",
    "    # Group by year and month, sum monthly burn\n",
    "    monthly_grouped = df_model.groupby(['year', 'month'])[model].sum()\n",
    "\n",
    "    # Group by year to get annual totals\n",
    "    yearly_totals = df_model.groupby('year')[model].sum()\n",
    "\n",
    "    # Normalize monthly by annual total (%)\n",
    "    monthly_percent = (monthly_grouped / yearly_totals).dropna() * 100\n",
    "\n",
    "    # Convert to DataFrame with 'year' and 'month' as columns\n",
    "    monthly_percent = monthly_percent.reset_index()\n",
    "\n",
    "    # Filter to relevant months\n",
    "    monthly_percent = monthly_percent[monthly_percent['month'].isin(months_to_include)]\n",
    "\n",
    "    for month in months_to_include:\n",
    "        data_month = monthly_percent[monthly_percent['month'] == month]\n",
    "\n",
    "        for start_year in range(data_month['year'].min(), data_month['year'].max() - window_size + 1, step):\n",
    "            window = data_month[(data_month['year'] >= start_year) & (data_month['year'] < start_year + window_size)]\n",
    "\n",
    "            if len(window) == window_size:\n",
    "                median_val = window[model].median()\n",
    "                summary_medians.append({\n",
    "                    'model': model,\n",
    "                    'month': month,\n",
    "                    'window_start': start_year,\n",
    "                    'window_end': start_year + window_size - 1,\n",
    "                    'median_percent': median_val\n",
    "                })\n",
    "\n",
    "# Create final summary DataFrame\n",
    "df_medians = pd.DataFrame(summary_medians)\n",
    "\n",
    "# Optional: Sort for readability\n",
    "df_medians.sort_values(by=['model', 'month', 'window_start'], inplace=True)\n",
    "\n",
    "# Save or inspect\n",
    "#df_medians.to_csv('/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/ba_median_percent_by_model_month_window.csv', index=False)\n",
    "df_medians[(df_medians['month']==5) & (df_medians['model'] == 'MRI_SSP370')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e4a83-f03f-4f4a-999d-388e874286ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Annual IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eeb224-f6b9-4af0-83d3-dd890f6d1b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Annual IQR - Set 25-yr periods\n",
    "#region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "#               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "#               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "#region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "#               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "#               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "# Step 1: Load all region files into a list of DataFrames\n",
    "models = ['ACCESS_SSP126', 'ACCESS_SSP245', 'ACCESS_SSP370',\n",
    "          'MRI_SSP126', 'MRI_SSP245', 'MRI_SSP370']\n",
    "\n",
    "# Placeholder for BA data\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    try:\n",
    "        # Load actual data\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled data\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Resample to annual and apply scaling\n",
    "        df_model_annual = df_model.resample('YE').sum().astype(float)\n",
    "\n",
    "        # Resample actual to annual\n",
    "        df_actual_annual = df_actual.resample('YE').sum()\n",
    "\n",
    "        # Combine both\n",
    "        df_ba = pd.concat([df_actual_annual, df_model_annual], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Combine all regions and sum across them\n",
    "df_ba_all = pd.concat(ba_dfs, axis=0).groupby(level=0).sum(min_count=1)\n",
    "df_ba_all.index = pd.to_datetime(df_ba_all.index)\n",
    "df_ba_all.sort_index(inplace=True)\n",
    "\n",
    "# Define the 25-year periods\n",
    "periods = {\n",
    "    '2025–2050': (2025, 2050),\n",
    "    '2051–2075': (2051, 2075),\n",
    "    '2076–2100': (2076, 2100)\n",
    "}\n",
    "\n",
    "# Calculate IQR for each model and time period\n",
    "iqr_records = []\n",
    "\n",
    "for model in models:\n",
    "    if model not in df_ba_all.columns:\n",
    "        continue\n",
    "\n",
    "    df_model = df_ba_all[[model]].copy()\n",
    "    df_model = df_model[df_model.index.year >= 2025]\n",
    "    df_model['year'] = df_model.index.year\n",
    "\n",
    "    # Get annual sums (already annual, so just group to be safe)\n",
    "    df_annual = df_model.groupby('year')[model].sum().reset_index()\n",
    "\n",
    "    for period_name, (start_year, end_year) in periods.items():\n",
    "        window = df_annual[(df_annual['year'] >= start_year) & (df_annual['year'] <= end_year)][model]\n",
    "        if not window.empty:\n",
    "            q1 = np.percentile(window, 25)\n",
    "            q3 = np.percentile(window, 75)\n",
    "            extreme = np.percentile(window, 95)\n",
    "            iqr = q3 - q1\n",
    "            iqr_records.append({\n",
    "                'Model': model,\n",
    "                'Period': period_name,\n",
    "                'IQR': iqr,\n",
    "                '95th percentile': extreme\n",
    "            })\n",
    "\n",
    "# Compile results into DataFrame\n",
    "df_iqr = pd.DataFrame(iqr_records)\n",
    "df_iqr\n",
    "\n",
    "# Optional export\n",
    "#output_path = '/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/annual_iqr_by_period.csv'\n",
    "#df_iqr.to_csv(output_path, index=False)\n",
    "#print(f\"Annual IQR results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f956270-880c-49fb-92c5-0a3caadc8e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Annual IQR - Moving 30-yr window\n",
    "#region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "#               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "#               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "#region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "#               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "#               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "#               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "#               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "#               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "# Step 1: Load all region files into a list of DataFrames\n",
    "models = ['ACCESS_SSP126', 'ACCESS_SSP245', 'ACCESS_SSP370',\n",
    "          'MRI_SSP126', 'MRI_SSP245', 'MRI_SSP370']\n",
    "\n",
    "# Placeholder for BA data\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    try:\n",
    "        # Load actual data\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled data\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Resample to annual and apply scaling\n",
    "        df_model_annual = df_model.resample('YE').sum().astype(float)\n",
    "\n",
    "        # Resample actual to annual\n",
    "        df_actual_annual = df_actual.resample('YE').sum()\n",
    "\n",
    "        # Combine both\n",
    "        df_ba = pd.concat([df_actual_annual, df_model_annual], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Combine all regions and sum across them\n",
    "df_ba_all = pd.concat(ba_dfs, axis=0).groupby(level=0).sum(min_count=1)\n",
    "df_ba_all.index = pd.to_datetime(df_ba_all.index)\n",
    "df_ba_all.sort_index(inplace=True)\n",
    "\n",
    "# Define the 25-year periods\n",
    "periods = {\n",
    "    '2025–2050': (2025, 2050),\n",
    "    '2051–2075': (2051, 2075),\n",
    "    '2076–2100': (2076, 2100)\n",
    "}\n",
    "\n",
    "# Calculate IQR for each model and time period\n",
    "iqr_records = []\n",
    "\n",
    "for model in models:\n",
    "    if model not in df_ba_all.columns:\n",
    "        continue\n",
    "\n",
    "    df_model = df_ba_all[[model]].copy()\n",
    "    df_model = df_model[df_model.index.year >= 2025]\n",
    "    df_model['year'] = df_model.index.year\n",
    "\n",
    "    # Get annual sums (already annual, so just group to be safe)\n",
    "    df_annual = df_model.groupby('year')[model].sum().reset_index()\n",
    "\n",
    "    # Apply 30-year moving window\n",
    "    for start_year in range(2025, 2071, 5):  # Last window is 2070–2099\n",
    "        end_year = start_year + 29\n",
    "        window = df_annual[(df_annual['year'] >= start_year) & (df_annual['year'] <= end_year)][model]\n",
    "\n",
    "        if not window.empty and len(window) >= 10:  # Require at least 10 years of data\n",
    "            q1 = np.percentile(window, 25)\n",
    "            q3 = np.percentile(window, 75)\n",
    "            iqr = q3 - q1\n",
    "            extreme = np.percentile(window, 95)\n",
    "\n",
    "            iqr_records.append({\n",
    "                'Model': model,\n",
    "                'StartYear': start_year,\n",
    "                'EndYear': end_year,\n",
    "                'IQR': iqr,\n",
    "                '95th percentile': extreme\n",
    "            })\n",
    "\n",
    "# Compile results into DataFrame\n",
    "df_iqr = pd.DataFrame(iqr_records)\n",
    "df_iqr\n",
    "\n",
    "# Optional export\n",
    "#output_path = '/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/annual_iqr_by_period.csv'\n",
    "#df_iqr.to_csv(output_path, index=False)\n",
    "#print(f\"Annual IQR results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6a2b0-0045-4cf9-a9b8-e08b84998767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Kendall trend analysis - 25-year periods\n",
    "#region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "#               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "#               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "#region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "#               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "#               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "# Step 1: Load all region files into a list of DataFrames\n",
    "models = ['ACCESS_SSP126', 'ACCESS_SSP245', 'ACCESS_SSP370',\n",
    "          'MRI_SSP126', 'MRI_SSP245', 'MRI_SSP370']\n",
    "\n",
    "# Placeholder for BA data\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    try:\n",
    "        # Load actual data\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled data\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Resample to annual and apply scaling\n",
    "        df_model_annual = df_model.resample('YE').sum().astype(float)\n",
    "\n",
    "        # Resample actual to annual\n",
    "        df_actual_annual = df_actual.resample('YE').sum()\n",
    "\n",
    "        # Combine both\n",
    "        df_ba = pd.concat([df_actual_annual, df_model_annual], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Combine all regions and sum across them\n",
    "df_ba_all = pd.concat(ba_dfs, axis=0).groupby(level=0).sum(min_count=1)\n",
    "df_ba_all.index = pd.to_datetime(df_ba_all.index)\n",
    "df_ba_all.sort_index(inplace=True)\n",
    "\n",
    "periods = {\n",
    "    '2025-2050': (2025, 2050),\n",
    "    '2051-2075': (2051, 2075),\n",
    "    '2076-2100': (2076, 2100),\n",
    "}\n",
    "\n",
    "trend_records = []\n",
    "\n",
    "for model in models:\n",
    "    if model not in df_ba_all.columns:\n",
    "        continue\n",
    "\n",
    "    df_model = df_ba_all[[model]].copy()\n",
    "    df_model['year'] = df_model.index.year\n",
    "    df_annual = df_model.groupby('year')[model].sum().reset_index()\n",
    "\n",
    "    for period_name, (start_year, end_year) in periods.items():\n",
    "        df_period = df_annual[(df_annual['year'] >= start_year) & (df_annual['year'] <= end_year)]\n",
    "\n",
    "        if len(df_period) < 10:\n",
    "            print(f\"Skipping {model} in {period_name}: insufficient data\")\n",
    "            continue\n",
    "\n",
    "        result = mk.original_test(df_period[model].values)\n",
    "\n",
    "        trend_records.append({\n",
    "            'Model': model,\n",
    "            'Period': period_name,\n",
    "            'Trend': result.trend,  # 'increasing', 'decreasing', 'no trend'\n",
    "            'PValue': result.p,\n",
    "            'Tau': result.Tau,      # Kendall's Tau\n",
    "            'Slope': result.slope,\n",
    "            'H': result.h           # True if trend is significant at alpha=0.05\n",
    "        })\n",
    "\n",
    "# Compile results\n",
    "df_mk = pd.DataFrame(trend_records)\n",
    "df_mk = df_mk[['Model', 'Period', 'Trend', 'H', 'PValue', 'Tau', 'Slope']]\n",
    "\n",
    "# Save or display\n",
    "#df_mk.to_csv('/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/mk_trends_2025_2100_by_period.csv', index=False)\n",
    "df_mk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27eac7-909f-4197-bee7-b92c1039acc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Moving window monthly PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f6e8d-9254-4a74-8fac-750648648392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Moving window monthly PDFs - not great\n",
    "#region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "#               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "#               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "#region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "#               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "#               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "#               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "#               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "#               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "models = ['ACCESS_SSP126', 'ACCESS_SSP245', 'ACCESS_SSP370',\n",
    "          'MRI_SSP126', 'MRI_SSP245', 'MRI_SSP370']\n",
    "\n",
    "# Step 1: Load all region files into a list of DataFrames\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    try:\n",
    "        # Load actual burned area\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled burned area\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Combine actual and model\n",
    "        df_ba = pd.concat([df_actual, df_model], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Step 2: Combine all DataFrames by aligning on time index\n",
    "df_ba_all = pd.concat(ba_dfs, axis=1)\n",
    "\n",
    "# Step 3: Sum each model's columns across regions\n",
    "df_ba = pd.DataFrame(index=df_ba_all.index)\n",
    "\n",
    "for model in models:\n",
    "    # Get all columns corresponding to this model from different regions\n",
    "    model_cols = [col for col in df_ba_all.columns if col == model]\n",
    "    if not model_cols:\n",
    "        continue\n",
    "    df_ba[model] = df_ba_all[model_cols].sum(axis=1)\n",
    "\n",
    "window_size = 30\n",
    "\n",
    "#for model in models:\n",
    "for model in ['ACCESS_SSP126']:\n",
    "    if model not in df_ba.columns:\n",
    "        continue\n",
    "\n",
    "    df_season = df_ba[[model]].copy().reset_index().rename(columns={'index': 'time'})\n",
    "    df_season['time'] = pd.to_datetime(df_season['time'])\n",
    "    df_season.set_index('time', inplace=True)\n",
    "    df_season = df_season.loc['2025-01-01':]\n",
    "\n",
    "    df_season['year'] = df_season.index.year\n",
    "    df_season['month'] = df_season.index.month\n",
    "\n",
    "    # Group by year and month\n",
    "    monthly_grouped = df_season.groupby(['year', 'month'])[model].sum()\n",
    "    yearly_grouped = df_season.groupby('year')[model].sum()\n",
    "\n",
    "    # Normalize by year total to get percentage\n",
    "    monthly_percent = monthly_grouped / monthly_grouped.index.get_level_values('year').map(yearly_grouped) * 100\n",
    "    monthly_percent = pd.DataFrame({model: monthly_percent}).reset_index()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        ax = axes[(month - 1) // 4, (month - 1) % 4]\n",
    "        data_month = monthly_percent[monthly_percent['month'] == month].copy()\n",
    "    \n",
    "        plotted = 0  # Counter for plotted KDEs\n",
    "    \n",
    "        for i, start_year in enumerate(range(data_month['year'].min(), data_month['year'].max() - window_size + 1)):\n",
    "            # To reduce clutter: plot every 5th window only\n",
    "            if i % 5 != 0:\n",
    "                continue\n",
    "    \n",
    "            window = data_month[\n",
    "                (data_month['year'] >= start_year) & \n",
    "                (data_month['year'] < start_year + window_size)\n",
    "            ]\n",
    "    \n",
    "            if len(window) == window_size:\n",
    "                sns.kdeplot(\n",
    "                    window[model].values, \n",
    "                    ax=ax, \n",
    "                    label=f\"{start_year}-{start_year + window_size - 1}\",\n",
    "                    fill=True, \n",
    "                    alpha=0.3,\n",
    "                    clip=(0, None),\n",
    "                    warn_singular=False\n",
    "                )\n",
    "                plotted += 1\n",
    "    \n",
    "        ax.set_title(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][month - 1])\n",
    "        ax.set_xlabel('% of Annual Burn')\n",
    "        if month in [1, 5, 9]:\n",
    "            ax.set_ylabel('Density')\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "    \n",
    "        if plotted > 0 and month == 1:\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    plt.suptitle(f'Monthly Burn Distribution (KDE) in 30-Year Windows\\nEurasia Boreal Ecoregions – {model}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    out_path = f'/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/KDE/ba_seasonal_kde_EUbor_{model}.png'\n",
    "    #plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabcaf0-8c0c-4bc8-9860-02ccfc99d4d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Grouped regions max month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd884b49-3cea-4bfa-9665-f385371213b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mappings  = [('alaspen', 'alapen'), ('centcan', 'cancsh'), ('cookinl', 'cookin'), ('copppla', 'copper'), ('eastcan', 'eastcf'), ('eashti', 'eashti'),\n",
    "               ('inteala', 'intlow'), ('mid-bor', 'midbor'), ('midwcan', 'midwes'), ('musklak', 'muslta'), ('nortcan', 'norths'), ('southud', 'sohudb'),\n",
    "               ('watshig', 'watson'), ('nortcor', 'norcor'), ('nortter', 'nwterr')] # N America boreal\n",
    "#region_mappings  = [('eastsib', 'eastsib'), ('icelbor', 'icelnd'), ('kamcmea', 'kamkurm'),\n",
    "#               ('kamctai', 'kamtaig'), ('nesibta', 'nesibta'), ('okhotai', 'okhman'), ('sakhisl', 'sakhtai'), ('trancon', 'trzconf'), ('westsib', 'westsib'),\n",
    "#               ('scanand', 'scrusta'), ('uralmon', 'uralfor')] # Eurasia boreal\n",
    "#region_mappings  = [('ahkland', 'ahklun'), ('berilow', 'berlow'), ('brooran', 'brookr'), ('kalanun', 'kalhar'),\n",
    "#               ('pacicoa', 'pacice'), ('novoisl', 'novoisl'), ('wranisl', 'wrangel'), ('alaseli', 'aleias'), ('arctcoa', 'arccoa'), ('arctfoo', 'arcfoo'),\n",
    "#               ('beriupl', 'berupl'), ('canalow', 'canlow'), ('davihig', 'davish'), ('canahig', 'canhig'), ('inteyuk', 'intalp'), ('canamid', 'canmid'),\n",
    "#               ('ogilalp', 'ogilvi'), ('tornmou', 'tornga'), ('kalste', 'kalste'), ('russarc', 'rusarc'), ('russber', 'rusbert'), ('chermou', 'cherski'),\n",
    "#               ('chukpen', 'chukchi'), ('kolapen', 'kolapen'), ('nortsib', 'nesibco'), ('nortrus', 'nwrunz'), ('scanmon', 'scambf'), ('taimsib', 'taicens'),\n",
    "#               ('tranbal', 'trzbald'), ('yamatun', 'yamalgy'), ('kamctun', 'kamtund')] # All tundra\n",
    "\n",
    "# List to store individual region DataFrames\n",
    "ba_dfs = []\n",
    "\n",
    "for region, region_model in region_mappings:\n",
    "    root = f'/home/users/clelland/Model/Analysis/CMIP and FWI time series/Ecoregion CSVs/{region}'\n",
    "\n",
    "    try:\n",
    "        # Load actual burned area\n",
    "        df_actual = pd.read_csv(\n",
    "            '/home/users/clelland/Model/Analysis/Fire actual 2001-2024.csv',\n",
    "            parse_dates=['date'], index_col='date'\n",
    "        )[[region]].rename(columns={region: 'Actual'})\n",
    "\n",
    "        # Load modeled burned area\n",
    "        df_model = pd.read_csv(\n",
    "            f'/home/users/clelland/Model/Analysis/Ecoregion plots combined/area_timeseries_{region_model}_all.csv',\n",
    "            parse_dates=['time'], index_col='time'\n",
    "        ).rename(columns={\n",
    "            'access 126': 'ACCESS_SSP126',\n",
    "            'access 245': 'ACCESS_SSP245',\n",
    "            'access 370': 'ACCESS_SSP370',\n",
    "            'mri 126': 'MRI_SSP126',\n",
    "            'mri 245': 'MRI_SSP245',\n",
    "            'mri 370': 'MRI_SSP370'\n",
    "        })\n",
    "\n",
    "        # Combine actual and model\n",
    "        df_ba = pd.concat([df_actual, df_model], axis=1)\n",
    "        ba_dfs.append(df_ba)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping region {region} ({region_model}) due to error: {e}\")\n",
    "\n",
    "# Combine and sum across all regions by timestamp\n",
    "df_ba_all = pd.concat(ba_dfs, axis=0).groupby(level=0).sum(min_count=1)\n",
    "df_ba_all.index = pd.to_datetime(df_ba_all.index)\n",
    "df_ba_all.sort_index(inplace=True)\n",
    "\n",
    "# Year range\n",
    "year_range = pd.Index(range(df_ba_all.index.year.min(), df_ba_all.index.year.max() + 1))\n",
    "\n",
    "# Get month of max BA, resolving ties using proximity to June\n",
    "max_month_df = pd.DataFrame(index=year_range)\n",
    "\n",
    "for column in df_ba_all.columns:\n",
    "    grouped = df_ba_all[column].groupby(df_ba_all.index.year)\n",
    "\n",
    "    def get_max_month(group):\n",
    "        if group.isna().all() or group.sum() == 0:\n",
    "            return np.nan\n",
    "        max_val = group.max()\n",
    "        max_months = group[group == max_val].index.month\n",
    "        return max_months[np.abs(max_months - 6).argmin()]  # Closest to June\n",
    "\n",
    "    max_month_df[column] = grouped.apply(get_max_month).reindex(year_range)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for column in max_month_df.columns:\n",
    "    # Raw line\n",
    "#    plt.plot(\n",
    "#        max_month_df.index,\n",
    "#        max_month_df[column],\n",
    "#        linestyle='-',\n",
    "#        label=f\"{column} raw\",\n",
    "#        alpha=0.4\n",
    "#    )\n",
    "    # 5-year rolling mean\n",
    "    rolling = max_month_df[column].rolling(window=5, min_periods=3).mean()\n",
    "    plt.plot(\n",
    "        rolling.index,\n",
    "        rolling,\n",
    "        label=column,\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "plt.yticks(ticks=range(1, 13), labels=[\n",
    "    'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'\n",
    "])\n",
    "plt.ylim(10.5, 2.5)\n",
    "plt.xticks(ticks=range(2000, 2101, 5))\n",
    "\n",
    "plt.title('5-year Rolling Mean of Month of Maximum Burned Area – Tundra ecoregions')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Month of Max Burn')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
    "           ncol=3, fancybox=True, shadow=True)\n",
    "plt.tight_layout()\n",
    "out_path = '/home/users/clelland/Model/Analysis/Summary stats/BA/Seasonality/ba_max_month_tundra.png'\n",
    "#plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ddcaf-81b3-4464-ab75-dab56cc84040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
